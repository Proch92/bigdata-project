stages:
  - build
  - submitjob
  - get_output
  - cleanup

buildjava:
  stage: build
  script:
    - "cd java"
    - "javac -Xdiags:verbose -classpath $(hadoop classpath) -d . JobOne.java"
    - "jar cf jobone.jar JobOne*.class"
    - "javac -Xdiags:verbose -classpath $(hadoop classpath) -d . JobTwo.java"
    - "jar cf jobtwo.jar JobTwo*.class"
  artifacts:
    paths:
      - java/jobone.jar
      - java/jobtwo.jar

submit:
  stage: submitjob
  allow_failure: true
  script:
#    - "gcloud dataproc jobs submit hadoop --region europe-west1 --cluster dataproc-cluster --jar java/jobone.jar -- JobOne data dataout1 london.csv outfile"
#    - "gcloud dataproc jobs submit hadoop --region europe-west1 --cluster dataproc-cluster --jar java/jobtwo.jar -- JobTwo data dataout2 london.csv outfile"
    - "hadoop jar java/jobone.jar JobOne data dataout1 london.csv outfile"
    - "hadoop jar java/jobtwo.jar JobTwo data dataout2 london.csv outfile"
    - "gcloud dataproc jobs submit pyspark --region europe-west1 --cluster dataproc-cluster python/job1.py"
    - "gcloud dataproc jobs submit pyspark --region europe-west1 --cluster dataproc-cluster python/job2.py"

retrieve:
  stage: get_output
  allow_failure: true
  script:
    - "hdfs dfs -getmerge dataout1/ out1.txt"
    - "hdfs dfs -getmerge dataout2/ out2.txt"
    - "hdfs dfs -getmerge outpy1/ outpy1.txt"
    - "hdfs dfs -getmerge outpy2/ outpy2.txt"
    - "cat out1.txt"
    - "cat out2.txt"
    - "cat outpy1.txt"
    - "cat outpy2.txt"
  artifacts:
    paths:
      - out1.txt
      - out2.txt
      - outpy1.txt
      - outpy2.txt

clean:
  stage: cleanup
  allow_failure: true
  script:
    - "hdfs dfs -rm -r temp1"
    - "hdfs dfs -rm -r temp2"
    - "hdfs dfs -rm -r dataout1"
    - "hdfs dfs -rm -r dataout2"